import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
from dotenv import load_dotenv
import time
import p
import json

# ì €ì¥í•  ë””ë ‰í† ë¦¬
SAVE_DIR = "data"
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)  # í´ë” ìƒì„±

# ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
CSV_PATH = os.path.join(SAVE_DIR, "news_data_full.csv")
PICKLE_PATH = os.path.join(SAVE_DIR, "news_data_full.pkl")
JSON_PATH = os.path.join(SAVE_DIR, "news_data_full.json")

# ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
for file in [CSV_PATH, PICKLE_PATH, JSON_PATH]:
    if os.path.exists(file):
        os.remove(file)

# ê¸°ë³¸ URL
BASE_URL = "https://www.yna.co.kr/market-plus/all/{}"

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ User-Agent ê°€ì ¸ì˜¤ê¸°
HEADERS = {
    "User-Agent": os.getenv("USER_AGENT", "Mozilla/5.0")  # ê¸°ë³¸ê°’ ì„¤ì • ê°€ëŠ¥
}


def get_news_data():
    all_data = []
    page = 1  # ì²« í˜ì´ì§€ë¶€í„° ì‹œì‘

    while True:
        url = BASE_URL.format(page)
        response = requests.get(url, headers=HEADERS)

        if response.status_code != 200:
            print(f"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
            break  # ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì¢…ë£Œ

        soup = BeautifulSoup(response.text, "html.parser")

        # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
        news_list = soup.select("div.list-type212 ul.list01 > li")

        # ë§ˆì§€ë§‰ í˜ì´ì§€ ê°ì§€ (li íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ)
        if not news_list:
            print(f"âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬: {page - 1} í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§ ì™„ë£Œ")
            break

        for news in news_list:
            data_cid = news.get("data-cid", "").strip()

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            img_tag = news.select_one("figure.img-con01 img")
            image_url = img_tag["src"] if img_tag else ""

            # ë‰´ìŠ¤ ì œëª©ê³¼ ë§í¬
            title_tag = news.select_one("strong.tit-wrap a span.title01")
            title = title_tag.get_text(strip=True) if title_tag else ""

            # ë‰´ìŠ¤ ìš”ì•½ (summary) ì¶”ì¶œ
            summary_tag = news.select_one("p.lead")
            summary = summary_tag.get_text(strip=True) if summary_tag else ""

            # ë‚ ì§œ ì¶”ì¶œ
            date_tag = news.select_one("span.txt-time")
            date = date_tag.get_text(strip=True) if date_tag else ""

            all_data.append({
                "data_cid": data_cid,
                "image_url": image_url,
                "title": title,
                "summary": summary,
                "date": date
            })

        print(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì™„ë£Œ")
        page += 1  # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        time.sleep(1)  # ìš”ì²­ ì†ë„ ì¡°ì ˆ

    return all_data


if __name__ == "__main__":
    news_data = get_news_data()

    # DataFrame ë³€í™˜
    df = pd.DataFrame(news_data)

    # CSV ì €ì¥
    df.to_csv(CSV_PATH, index=False, encoding="utf-8-sig")

    # Pickle ì €ì¥
    with open(PICKLE_PATH, "wb") as f:
        pickle.dump(news_data, f)

    # JSON ì €ì¥
    with open(JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=4)

    print("âœ… ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ, ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {SAVE_DIR}/news_data_full.csv, .pkl, .json")
